Usage: wawenet infile [options]

INPUTS

--infile is either a .wav file or a .txt file where each line specifies
a suitable .wav file (In this second case, the listed .wav files will be
processed in sequence).

A suitable wav file: 
   is uncompressed
   has sample rate 8, 16, 24, 32, or 48k 
   contains at least 3 seconds of speech
To best match the designed scope of WAWEnets, it should also have at 
least .50 speech activity factor and an active speech level near
26 dB below clipping (see level normalization feature below).
The native sample rate for WAWEnets is 16k so files with rates 8, 24, 32,
or 48k rate will be converted internally before processing.

-- use option -m M to specify a WAWEnet mode. The integer M
specifies the WAWEnet trained using a specific full-reference target.
M=1: WAWEnet trained using WB-PESQ [2] target values (Default)
M=2: WAWEnet trained using POLQA [3] target values
M=3: WAWEnet trained using PEMO [4] target values
M=4: WAWEnet trained using STOI [5] target values

-- use option -l L to specify internal level normalization of 
.wav file contents to 26 dB below clipping.
L=0: normalization off
L=1: normalization on (Default)

-- use option -s S to specify segment step (stride).
WAWEnet requires a full 3 seconds of signal to generate a result. 
If the input speech .wav file is longer than 3 seconds
multiple results may be produced. S specifies the number of samples to
move ahead in the file when extracting the next segment.
The default value of 48,000 gives zero overlap between
segments. Using this default any input less than 6 seconds will produce one
result, based on just the first 3 seconds A 6 second input will produce two
results. If S = 24,000 for example, segment overlap will be 50%, a 
4.5 second input will produce 2 results and a 6 second input will produce
3 results. In all cases an additional final value is produced. That 
value is the mean of all segment results that are associated with 
segments that have speech activity factor greater than 0.45. If there are
no such segments, the final result is NaN.

-- use option -c C to specify a channel number to use in the 
case of multi-channel .wav files. Default is C = 1.

-- use option -o 'myFile.txt' to specify a text file that
captures WAWEnet results for each processed .wav file on a new line.
If the file exists it will be appended. The extension .txt will be added
as needed. Default is no .txt file generated.

OUTPUTS

The output for each of the N speech signals processed is in the format:

[wavfile] [channel] [sample_rate] [duration] [active_level] [speech_activity]
    [level_normalization] [segment_step_size] [WAWEnet_mode] [model_prediction]

- wavfile is the filename that has been processed
- channel is the channel of wavfile that has been processed
- sample_rate native sample rate of the wavfile
- duration duration of wavfile in seconds
- active_level active speech level of the last processed segment of wavfile 
  in dB below overload
- speech_activity is the speech activity factor of the last processed segment
  of wavfile
- level_normalization reflects whether wavfile was normalized during processing
- segment_step_size reflects the segment step (stride) used to process wavfile
- WAWEnet_mode is the mode wavfile has been processed with
- model_prediction output value (or values, if >1 segment was analyzed) produced
  by WAWEnet for wavfile, plus a final value that is the mean over all segments
  that have a speech activity factor greater than 0.45

References:
[1] A. A. Catellier and S.D. Voran, "WAWEnets: A No-Reference 
Convolutional Waveform-Based Approach to Estimating Narrowband and 
Wideband Speech Quality," in Proc. 45th IEEE International Conference
on Acoustics, Speech and Signal Processing, May 2020.
[2] ITU-T Recommendation P.862, "Perceptual evaluation of speech quality
(PESQ)," Geneva, 2001.
[3] ITU-T Recommendation P.863, "Perceptual objective listening quality
analysis," Geneva, 2018.
[4] R. Huber and B. Kollmeier, "PEMO-Q - A new method for objective audio
quality assessment using a model of auditory perception," IEEE Trans.
ASLP, vol. 14, no. 6, pp. 1902-1911, Nov. 2006.
[5] C. H. Taal, R. C. Hendriks, R. Heusdens, and J. Jensen, "An algorithm
for intelligibility prediction of time-frequency weighted noisy speech,"
IEEE Trans. ASLP, vol. 19, no. 7, pp. 2125-2136, Sep. 2011.